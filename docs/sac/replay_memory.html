<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.1" />
<title>sac.replay_memory API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sac.replay_memory</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import random
import numpy as np
import pickle
from pickle import UnpicklingError
from pathlib import Path

class ReplayMemory:
    &#34;&#34;&#34;
    The SAC algorithm uses an experience replay buffer to learn from past samples. This class allows to add transitions 
    to the replay buffer, save the replay buffer, and to sample past transitions from the replay buffer.  
    
    **Parameters**:  
    
    - **capacity** *(int)*:  Defines the size of the replay buffer.
    &#34;&#34;&#34;
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        &#34;&#34;&#34;
        Appends a transistion to the replay buffer.  
        
        **Parameters**:  
        
        - **state** *(array)*:  Contains thirty-two float32 values that represent the agent&#39;s observable environment before 
        executing its action.
        - **action** *(array)*:  Contains three float32 values that allow to steer, accelerate and decelerate the vehicle.
        - **reward** *(float)*:  Represents the reward that the agent receives from the environment for its action.
        - **next_state** *(array)*:  Contains thirty-two float32 values that represent the agent&#39;s observable environment after 
        executing its action.
        - **done** *(boolean)*:  Changes to *True* when the number of tiles that the agent visited euqals the current track&#39;s 
        tile count or the agent goes to far astray and consequently aborts the current episode.
        &#34;&#34;&#34;
        if len(self.buffer) &lt; self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        &#34;&#34;&#34;
        Outputs samples contained in the replay buffer.  
        
        **Parameters**:  
        
        - **batch_size** *(int)*:  Defines how many samples to output.
        
        **Output**:  
        
        - **state** *(array)*: Contains *batch_size* state arrays
        - **action** *(array)*: Contains *batch_size* action arrays
        - **reward** *(array)*: Contains *batch_size* reward values
        - **next_state** *(array)*: Contains *batch_size* next_state arrays
        - **done** *(array)*: Contains *batch_size* done values
        &#34;&#34;&#34;
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done

    def save(self, path: str):
        &#34;&#34;&#34;
        Option to save the replay buffer to allow to pick up model training in the future. 
        The buffer is saved to path &#34;/memory/buffer&#34; as a Pickle file.
        
        **Parameters**:  
        
        - **path** *(String)*:  Defined as &#34;buffer&#34; in the calling method train().
        &#34;&#34;&#34;
        print(f&#34;Saving replay buffer to {path}.&#34;)
        save_dir = Path(&#34;memory/&#34;)
        if not save_dir.exists():
            save_dir.mkdir() 
        with open((save_dir/path).with_suffix(&#34;.pkl&#34;), &#34;wb&#34;) as fp:
            pickle.dump(self.buffer, fp)

    def load(self, path: str):
        &#34;&#34;&#34;
        Option to load a previously saved replay buffer from a Pickle file.  
        
        ## Parameters:  
        
        - **path** *(String)*:  Path to Pickle file.
        &#34;&#34;&#34;
        try:
            with open(path, &#34;rb&#34;) as fp:
                mem = pickle.load(fp)
                assert len(mem) == self.capacity, &#34;Capacity of the replay buffer and length of the loaded memory don&#39;t match!&#34;
                self.buffer = mem
            print(f&#34;Loaded saved replay buffer from {path}.&#34;)
        except UnpicklingError:
            raise TypeError(&#34;This file doesn&#39;t contain a pickled list!&#34;)

    def __len__(self):
        return len(self.buffer)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="sac.replay_memory.ReplayMemory"><code class="flex name class">
<span>class <span class="ident">ReplayMemory</span></span>
<span>(</span><span>capacity)</span>
</code></dt>
<dd>
<section class="desc"><p>The SAC algorithm uses an experience replay buffer to learn from past samples. This class allows to add transitions
to the replay buffer, save the replay buffer, and to sample past transitions from the replay buffer.
</p>
<p><strong>Parameters</strong>:
</p>
<ul>
<li><strong>capacity</strong> <em>(int)</em>:
Defines the size of the replay buffer.</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReplayMemory:
    &#34;&#34;&#34;
    The SAC algorithm uses an experience replay buffer to learn from past samples. This class allows to add transitions 
    to the replay buffer, save the replay buffer, and to sample past transitions from the replay buffer.  
    
    **Parameters**:  
    
    - **capacity** *(int)*:  Defines the size of the replay buffer.
    &#34;&#34;&#34;
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        &#34;&#34;&#34;
        Appends a transistion to the replay buffer.  
        
        **Parameters**:  
        
        - **state** *(array)*:  Contains thirty-two float32 values that represent the agent&#39;s observable environment before 
        executing its action.
        - **action** *(array)*:  Contains three float32 values that allow to steer, accelerate and decelerate the vehicle.
        - **reward** *(float)*:  Represents the reward that the agent receives from the environment for its action.
        - **next_state** *(array)*:  Contains thirty-two float32 values that represent the agent&#39;s observable environment after 
        executing its action.
        - **done** *(boolean)*:  Changes to *True* when the number of tiles that the agent visited euqals the current track&#39;s 
        tile count or the agent goes to far astray and consequently aborts the current episode.
        &#34;&#34;&#34;
        if len(self.buffer) &lt; self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        &#34;&#34;&#34;
        Outputs samples contained in the replay buffer.  
        
        **Parameters**:  
        
        - **batch_size** *(int)*:  Defines how many samples to output.
        
        **Output**:  
        
        - **state** *(array)*: Contains *batch_size* state arrays
        - **action** *(array)*: Contains *batch_size* action arrays
        - **reward** *(array)*: Contains *batch_size* reward values
        - **next_state** *(array)*: Contains *batch_size* next_state arrays
        - **done** *(array)*: Contains *batch_size* done values
        &#34;&#34;&#34;
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done

    def save(self, path: str):
        &#34;&#34;&#34;
        Option to save the replay buffer to allow to pick up model training in the future. 
        The buffer is saved to path &#34;/memory/buffer&#34; as a Pickle file.
        
        **Parameters**:  
        
        - **path** *(String)*:  Defined as &#34;buffer&#34; in the calling method train().
        &#34;&#34;&#34;
        print(f&#34;Saving replay buffer to {path}.&#34;)
        save_dir = Path(&#34;memory/&#34;)
        if not save_dir.exists():
            save_dir.mkdir() 
        with open((save_dir/path).with_suffix(&#34;.pkl&#34;), &#34;wb&#34;) as fp:
            pickle.dump(self.buffer, fp)

    def load(self, path: str):
        &#34;&#34;&#34;
        Option to load a previously saved replay buffer from a Pickle file.  
        
        ## Parameters:  
        
        - **path** *(String)*:  Path to Pickle file.
        &#34;&#34;&#34;
        try:
            with open(path, &#34;rb&#34;) as fp:
                mem = pickle.load(fp)
                assert len(mem) == self.capacity, &#34;Capacity of the replay buffer and length of the loaded memory don&#39;t match!&#34;
                self.buffer = mem
            print(f&#34;Loaded saved replay buffer from {path}.&#34;)
        except UnpicklingError:
            raise TypeError(&#34;This file doesn&#39;t contain a pickled list!&#34;)

    def __len__(self):
        return len(self.buffer)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="sac.replay_memory.ReplayMemory.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, path: str)</span>
</code></dt>
<dd>
<section class="desc"><p>Option to load a previously saved replay buffer from a Pickle file.
</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li><strong>path</strong> <em>(String)</em>:
Path to Pickle file.</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, path: str):
    &#34;&#34;&#34;
    Option to load a previously saved replay buffer from a Pickle file.  
    
    ## Parameters:  
    
    - **path** *(String)*:  Path to Pickle file.
    &#34;&#34;&#34;
    try:
        with open(path, &#34;rb&#34;) as fp:
            mem = pickle.load(fp)
            assert len(mem) == self.capacity, &#34;Capacity of the replay buffer and length of the loaded memory don&#39;t match!&#34;
            self.buffer = mem
        print(f&#34;Loaded saved replay buffer from {path}.&#34;)
    except UnpicklingError:
        raise TypeError(&#34;This file doesn&#39;t contain a pickled list!&#34;)</code></pre>
</details>
</dd>
<dt id="sac.replay_memory.ReplayMemory.push"><code class="name flex">
<span>def <span class="ident">push</span></span>(<span>self, state, action, reward, next_state, done)</span>
</code></dt>
<dd>
<section class="desc"><p>Appends a transistion to the replay buffer.
</p>
<p><strong>Parameters</strong>:
</p>
<ul>
<li><strong>state</strong> <em>(array)</em>:
Contains thirty-two float32 values that represent the agent's observable environment before
executing its action.</li>
<li><strong>action</strong> <em>(array)</em>:
Contains three float32 values that allow to steer, accelerate and decelerate the vehicle.</li>
<li><strong>reward</strong> <em>(float)</em>:
Represents the reward that the agent receives from the environment for its action.</li>
<li><strong>next_state</strong> <em>(array)</em>:
Contains thirty-two float32 values that represent the agent's observable environment after
executing its action.</li>
<li><strong>done</strong> <em>(boolean)</em>:
Changes to <em>True</em> when the number of tiles that the agent visited euqals the current track's
tile count or the agent goes to far astray and consequently aborts the current episode.</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def push(self, state, action, reward, next_state, done):
    &#34;&#34;&#34;
    Appends a transistion to the replay buffer.  
    
    **Parameters**:  
    
    - **state** *(array)*:  Contains thirty-two float32 values that represent the agent&#39;s observable environment before 
    executing its action.
    - **action** *(array)*:  Contains three float32 values that allow to steer, accelerate and decelerate the vehicle.
    - **reward** *(float)*:  Represents the reward that the agent receives from the environment for its action.
    - **next_state** *(array)*:  Contains thirty-two float32 values that represent the agent&#39;s observable environment after 
    executing its action.
    - **done** *(boolean)*:  Changes to *True* when the number of tiles that the agent visited euqals the current track&#39;s 
    tile count or the agent goes to far astray and consequently aborts the current episode.
    &#34;&#34;&#34;
    if len(self.buffer) &lt; self.capacity:
        self.buffer.append(None)
    self.buffer[self.position] = (state, action, reward, next_state, done)
    self.position = (self.position + 1) % self.capacity</code></pre>
</details>
</dd>
<dt id="sac.replay_memory.ReplayMemory.sample"><code class="name flex">
<span>def <span class="ident">sample</span></span>(<span>self, batch_size)</span>
</code></dt>
<dd>
<section class="desc"><p>Outputs samples contained in the replay buffer.
</p>
<p><strong>Parameters</strong>:
</p>
<ul>
<li><strong>batch_size</strong> <em>(int)</em>:
Defines how many samples to output.</li>
</ul>
<p><strong>Output</strong>:
</p>
<ul>
<li><strong>state</strong> <em>(array)</em>: Contains <em>batch_size</em> state arrays</li>
<li><strong>action</strong> <em>(array)</em>: Contains <em>batch_size</em> action arrays</li>
<li><strong>reward</strong> <em>(array)</em>: Contains <em>batch_size</em> reward values</li>
<li><strong>next_state</strong> <em>(array)</em>: Contains <em>batch_size</em> next_state arrays</li>
<li><strong>done</strong> <em>(array)</em>: Contains <em>batch_size</em> done values</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample(self, batch_size):
    &#34;&#34;&#34;
    Outputs samples contained in the replay buffer.  
    
    **Parameters**:  
    
    - **batch_size** *(int)*:  Defines how many samples to output.
    
    **Output**:  
    
    - **state** *(array)*: Contains *batch_size* state arrays
    - **action** *(array)*: Contains *batch_size* action arrays
    - **reward** *(array)*: Contains *batch_size* reward values
    - **next_state** *(array)*: Contains *batch_size* next_state arrays
    - **done** *(array)*: Contains *batch_size* done values
    &#34;&#34;&#34;
    batch = random.sample(self.buffer, batch_size)
    state, action, reward, next_state, done = map(np.stack, zip(*batch))
    return state, action, reward, next_state, done</code></pre>
</details>
</dd>
<dt id="sac.replay_memory.ReplayMemory.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, path: str)</span>
</code></dt>
<dd>
<section class="desc"><p>Option to save the replay buffer to allow to pick up model training in the future.
The buffer is saved to path "/memory/buffer" as a Pickle file.</p>
<p><strong>Parameters</strong>:
</p>
<ul>
<li><strong>path</strong> <em>(String)</em>:
Defined as "buffer" in the calling method train().</li>
</ul></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self, path: str):
    &#34;&#34;&#34;
    Option to save the replay buffer to allow to pick up model training in the future. 
    The buffer is saved to path &#34;/memory/buffer&#34; as a Pickle file.
    
    **Parameters**:  
    
    - **path** *(String)*:  Defined as &#34;buffer&#34; in the calling method train().
    &#34;&#34;&#34;
    print(f&#34;Saving replay buffer to {path}.&#34;)
    save_dir = Path(&#34;memory/&#34;)
    if not save_dir.exists():
        save_dir.mkdir() 
    with open((save_dir/path).with_suffix(&#34;.pkl&#34;), &#34;wb&#34;) as fp:
        pickle.dump(self.buffer, fp)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sac" href="index.html">sac</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="sac.replay_memory.ReplayMemory" href="#sac.replay_memory.ReplayMemory">ReplayMemory</a></code></h4>
<ul class="">
<li><code><a title="sac.replay_memory.ReplayMemory.load" href="#sac.replay_memory.ReplayMemory.load">load</a></code></li>
<li><code><a title="sac.replay_memory.ReplayMemory.push" href="#sac.replay_memory.ReplayMemory.push">push</a></code></li>
<li><code><a title="sac.replay_memory.ReplayMemory.sample" href="#sac.replay_memory.ReplayMemory.sample">sample</a></code></li>
<li><code><a title="sac.replay_memory.ReplayMemory.save" href="#sac.replay_memory.ReplayMemory.save">save</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>